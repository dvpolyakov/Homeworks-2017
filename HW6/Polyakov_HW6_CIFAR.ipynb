{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:08:00.0)\n"
     ]
    }
   ],
   "source": [
    "from lasagne import init\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "#from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "import lasagne.nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "from lasagne.layers import Conv2DLayer, MaxPool2DLayer, FeaturePoolLayer, DropoutLayer\n",
    "from __future__ import print_function\n",
    "from lasagne import init\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "#from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "import lasagne.nonlinearities\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберите нейронку: \n",
    "- Many times x (Conv+Pool)\n",
    "- Many small convolutions like 3x3\n",
    "- Batch Norm \n",
    "- Residual Connection\n",
    "- Data Augmentation \n",
    "- Learning rate Schedule \n",
    "- ...\n",
    "\n",
    "### Для вдохновения \n",
    "- http://torch.ch/blog/2015/07/30/cifar.html\n",
    "- https://github.com/szagoruyko/wide-residual-networks \n",
    "\n",
    "### Самое интересное\n",
    "- Для сдачи задания нужно набрать на точность тесте > **92.5**% (это займет много времени, торопитесь :) )\n",
    "- Для получения бонусных баллов > **95.0**%\n",
    "- Будет очень хорошо если вы придумаете свою архитектуру или сможете обучить что-то из вышеперечисленного :)\n",
    "- А для обучения всего этого добра вам будет куда удобнее использовать GPU на Amazon \n",
    "    - Инструкция https://github.com/persiyanov/ml-mipt/tree/master/amazon-howto \n",
    "    - Вам помогут tmux, CuDNN, ssh tunnel, nvidia-smi, ... \n",
    "    - Have fun :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### За основу возьмем архитектуру из этой статьи: https://github.com/Lasagne/Recipes/blob/master/papers/deep_residual_learning/Deep_Residual_Learning_CIFAR-10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для увеличения выборки сделаем data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy as np\n",
    "\n",
    "# random example images\n",
    "images = np.random.randint(0, 255, (16, 128, 128, 3), dtype=np.uint8)\n",
    "\n",
    "st = lambda aug: iaa.Sometimes(0.3, aug)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n",
    "        iaa.Flipud(0.5), # vertically flip 50% of all images\n",
    "        st(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
    "        st(iaa.Crop(percent=(0, 0.1))), # crop images by 0-10% of their height/width\n",
    "        st(iaa.GaussianBlur((0, 3.0))), # blur images with a sigma between 0 and 3.0\n",
    "        st(iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5))), # sharpen images\n",
    "        st(iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0))), # emboss images\n",
    "        # search either for all edges or for directed edges\n",
    "        st(iaa.Sometimes(0.5,\n",
    "            iaa.EdgeDetect(alpha=(0, 0.7)),\n",
    "            iaa.DirectedEdgeDetect(alpha=(0, 0.7), direction=(0.0, 1.0)),\n",
    "        )),\n",
    "        st(iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.2), per_channel=0.5)), # add gaussian noise to images\n",
    "        st(iaa.Dropout((0.0, 0.1), per_channel=0.5)), # randomly remove up to 10% of the pixels\n",
    "        st(iaa.Invert(0.25, per_channel=True)), # invert color channels\n",
    "        st(iaa.Add((-10, 10), per_channel=0.5)), # change brightness of images (by -10 to 10 of original value)\n",
    "        st(iaa.Multiply((0.5, 1.5), per_channel=0.5)), # change brightness of images (50-150% of original value)\n",
    "        st(iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5)), # improve or worsen the contrast\n",
    "        st(iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n",
    "            translate_px={\"x\": (-16, 16), \"y\": (-16, 16)}, # translate by -16 to +16 pixels (per axis)\n",
    "            rotate=(-45, 45), # rotate by -45 to +45 degrees\n",
    "            shear=(-16, 16), # shear by -16 to +16 degrees\n",
    "            order=ia.ALL, # use any of scikit-image's interpolation methods\n",
    "            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
    "            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
    "        )),\n",
    "        st(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)) # apply elastic transformations with random strengths\n",
    "    ],\n",
    "    random_order=True # do all of the above in random order\n",
    ")\n",
    "\n",
    "images_aug = seq.augment_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_augmented_data(iter_num, x, sequoya):\n",
    "    x_new = [x]\n",
    "    for _ in range(iter_num):\n",
    "        x_new.append(sequoya.augment_images(x))\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(5):\n",
    "      d = unpickle('cifar-10-batches-py/data_batch_'+`j+1`)\n",
    "      x = d['data']\n",
    "      y = d['labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "    x = seq.augment_images(x)\n",
    "    \n",
    "    # subtract per-pixel mean\n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    #pickle.dump(pixel_mean, open(\"cifar10-pixel_mean.pkl\",\"wb\"))\n",
    "    x -= pixel_mean\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_train_flip = X_train[:,:,:,::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train,X_train_flip),axis=0)\n",
    "    Y_train = np.concatenate((Y_train,Y_train_flip),axis=0)\n",
    "\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        X_test = lasagne.utils.floatX(X_test),\n",
    "        Y_test = Y_test.astype('int32'),)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть обучалась около 7 часов (90 эпох). Запускать ее снова нет ввозможности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_data():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j in range(5):\n",
    "      d = unpickle('cifar-10-batches-py/data_batch_'+`j+1`)\n",
    "      x = d['data']\n",
    "      y = d['labels']\n",
    "      xs.append(x)\n",
    "      ys.append(y)\n",
    "\n",
    "    d = unpickle('cifar-10-batches-py/test_batch')\n",
    "    xs.append(d['data'])\n",
    "    ys.append(d['labels'])\n",
    "\n",
    "    x = np.concatenate(xs)/np.float32(255)\n",
    "    y = np.concatenate(ys)\n",
    "    x = np.dstack((x[:, :1024], x[:, 1024:2048], x[:, 2048:]))\n",
    "    x = x.reshape((x.shape[0], 32, 32, 3)).transpose(0,3,1,2)\n",
    "\n",
    "    x = seq.augment_images(x)\n",
    "    \n",
    "    # subtract per-pixel mean\n",
    "    pixel_mean = np.mean(x[0:50000],axis=0)\n",
    "    #pickle.dump(pixel_mean, open(\"cifar10-pixel_mean.pkl\",\"wb\"))\n",
    "    x -= pixel_mean\n",
    "\n",
    "    # create mirrored images\n",
    "    X_train = x[0:50000,:,:,:]\n",
    "    Y_train = y[0:50000]\n",
    "    X_train_flip = X_train[:,:,:,::-1]\n",
    "    Y_train_flip = Y_train\n",
    "    X_train = np.concatenate((X_train,X_train_flip),axis=0)\n",
    "    Y_train = np.concatenate((Y_train,Y_train_flip),axis=0)\n",
    "\n",
    "    X_test = x[50000:,:,:,:]\n",
    "    Y_test = y[50000:]\n",
    "\n",
    "    return dict(\n",
    "        X_train=lasagne.utils.floatX(X_train),\n",
    "        Y_train=Y_train.astype('int32'),\n",
    "        X_test = lasagne.utils.floatX(X_test),\n",
    "        Y_test = Y_test.astype('int32'),)\n",
    "\n",
    "# ##################### Build the neural network model #######################\n",
    "\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer\n",
    "#from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import GlobalPoolLayer\n",
    "from lasagne.layers import PadLayer\n",
    "from lasagne.layers import ExpressionLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, rectify\n",
    "from lasagne.layers import batch_norm\n",
    "\n",
    "def build_cnn(input_var=None, n=5):\n",
    "    \n",
    "    # create a residual learning building block with two stacked 3x3 convlayers as in paper\n",
    "    def residual_block(l, increase_dim=False, projection=False):\n",
    "        input_num_filters = l.output_shape[1]\n",
    "        if increase_dim:\n",
    "            first_stride = (2,2)\n",
    "            out_num_filters = input_num_filters*2\n",
    "        else:\n",
    "            first_stride = (1,1)\n",
    "            out_num_filters = input_num_filters\n",
    "\n",
    "        stack_1 = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(3,3), stride=first_stride, nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "        stack_2 = batch_norm(ConvLayer(stack_1, num_filters=out_num_filters, filter_size=(3,3), stride=(1,1), nonlinearity=None, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "        \n",
    "        # add shortcut connections\n",
    "        if increase_dim:\n",
    "            if projection:\n",
    "                # projection shortcut, as option B in paper\n",
    "                projection = batch_norm(ConvLayer(l, num_filters=out_num_filters, filter_size=(1,1), stride=(2,2), nonlinearity=None, pad='same', b=None, flip_filters=False))\n",
    "                block = NonlinearityLayer(ElemwiseSumLayer([stack_2, projection]),nonlinearity=rectify)\n",
    "            else:\n",
    "                # identity shortcut, as option A in paper\n",
    "                identity = ExpressionLayer(l, lambda X: X[:, :, ::2, ::2], lambda s: (s[0], s[1], s[2]//2, s[3]//2))\n",
    "                padding = PadLayer(identity, [out_num_filters//4,0,0], batch_ndim=1)\n",
    "                block = NonlinearityLayer(ElemwiseSumLayer([stack_2, padding]),nonlinearity=rectify)\n",
    "        else:\n",
    "            block = NonlinearityLayer(ElemwiseSumLayer([stack_2, l]),nonlinearity=rectify)\n",
    "        \n",
    "        return block\n",
    "\n",
    "    # Building the network\n",
    "    l_in = InputLayer(shape=(None, 3, 32, 32), input_var=input_var)\n",
    "\n",
    "    # first layer, output is 16 x 32 x 32\n",
    "    l = batch_norm(ConvLayer(l_in, num_filters=16, filter_size=(3,3), stride=(1,1), nonlinearity=rectify, pad='same', W=lasagne.init.HeNormal(gain='relu'), flip_filters=False))\n",
    "    \n",
    "    # first stack of residual blocks, output is 16 x 32 x 32\n",
    "    for _ in range(n):\n",
    "        l = residual_block(l)\n",
    "\n",
    "    # second stack of residual blocks, output is 32 x 16 x 16\n",
    "    l = residual_block(l, increase_dim=True)\n",
    "    for _ in range(1,n):\n",
    "        l = residual_block(l)\n",
    "\n",
    "    # third stack of residual blocks, output is 64 x 8 x 8\n",
    "    l = residual_block(l, increase_dim=True)\n",
    "    for _ in range(1,n):\n",
    "        l = residual_block(l)\n",
    "    \n",
    "    # average pooling\n",
    "    l = GlobalPoolLayer(l)\n",
    "\n",
    "    # fully connected layer\n",
    "    network = DenseLayer(\n",
    "            l, num_units=10,\n",
    "            W=lasagne.init.HeNormal(),\n",
    "            nonlinearity=softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False, augment=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        if augment:\n",
    "            # as in paper : \n",
    "            # pad feature arrays with 4 pixels on each side\n",
    "            # and do random cropping of 32x32\n",
    "            padded = np.pad(inputs[excerpt],((0,0),(0,0),(4,4),(4,4)),mode='constant')\n",
    "            random_cropped = np.zeros(inputs[excerpt].shape, dtype=np.float32)\n",
    "            crops = np.random.random_integers(0,high=8,size=(batchsize,2))\n",
    "            for r in range(batchsize):\n",
    "                random_cropped[r,:,:,:] = padded[r,:,crops[r,0]:(crops[r,0]+32),crops[r,1]:(crops[r,1]+32)]\n",
    "            inp_exc = random_cropped\n",
    "        else:\n",
    "            inp_exc = inputs[excerpt]\n",
    "\n",
    "        yield inp_exc, targets[excerpt]\n",
    "\n",
    "# ############################## Main program ################################\n",
    "\n",
    "\n",
    "n=5\n",
    "num_epochs=90\n",
    "model=None\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./cifar-10-batches-py\"):\n",
    "    print(\"CIFAR-10 dataset can not be found. Please download the dataset from 'https://www.cs.toronto.edu/~kriz/cifar.html'.\")\n",
    "    print(':(', 1./0)\n",
    "    \n",
    "    #     return\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "data = load_data()\n",
    "X_train = data['X_train']\n",
    "Y_train = data['Y_train']\n",
    "X_test = data['X_test']\n",
    "Y_test = data['Y_test']\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var, n)\n",
    "print(\"number of parameters in model: %d\" % lasagne.layers.count_params(network, trainable=True))\n",
    "\n",
    "if model is None:\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    # add weight decay\n",
    "    all_layers = lasagne.layers.get_all_layers(network)\n",
    "    l2_penalty = lasagne.regularization.regularize_layer_params(all_layers, lasagne.regularization.l2) * 0.0001\n",
    "    loss = loss + l2_penalty\n",
    "\n",
    "    # Create update expressions for training\n",
    "    # Stochastic Gradient Descent (SGD) with momentum\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    lr = 0.1\n",
    "    sh_lr = theano.shared(lasagne.utils.floatX(lr))\n",
    "    updates = lasagne.updates.momentum(\n",
    "            loss, params, learning_rate=sh_lr, momentum=0.9)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Create a loss expression for validation/testing\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "if model is None:\n",
    "    # launch the training loop\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # shuffle training data\n",
    "        train_indices = np.arange(100000)\n",
    "        np.random.shuffle(train_indices)\n",
    "        X_train = X_train[train_indices,:,:,:]\n",
    "        Y_train = Y_train[train_indices]\n",
    "\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, Y_train, 128, shuffle=True, augment=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "        # adjust learning rate as in paper\n",
    "        # 32k and 48k iterations should be roughly equivalent to 41 and 61 epochs\n",
    "        if (epoch+1) == 41 or (epoch+1) == 61:\n",
    "            new_lr = sh_lr.get_value() * 0.1\n",
    "            print(\"New LR:\"+str(new_lr))\n",
    "            sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "\n",
    "    # dump the network weights to a file :\n",
    "    np.savez('cifar10_deep_residual_model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "else:\n",
    "    # load network weights from model file\n",
    "    with np.load(model) as f:\n",
    "         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "# Calculate validation error of model:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, Y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Барабанная дробь! Смотрим результат сети на тесте после 90 эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.327143\n",
      "  test accuracy:\t\t92.65 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В общем, победа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">First of all -- Checking Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Чем отличаются современные сверточные сети от сетей 5 летней давности?\n",
    "\n",
    "Современные сети стали глубже.\n",
    "\n",
    "**Вопрос 2**: Какие неприятности могут возникнуть во время обучения современных нейросетей?\n",
    "\n",
    "Если они слишком глубокие, то может затухать градиент. Сеть может переобучаться.\n",
    "\n",
    "**Вопрос 3**: У вас есть очень маленький датасет из 100 картинок, классификация, но вы очень хотите использовать нейросеть, какие неприятности вас ждут и как их решить? что делать если первый вариант  решения не заработает?\n",
    "\n",
    "сделать data augmentation\n",
    "\n",
    "Если ничего не получится, пойду на толоку и заставлю больше размечать картинки\n",
    "\n",
    "**Вопрос 4**: Как сделать стайл трансфер для музыки? oO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполните форму\n",
    "\n",
    "https://goo.gl/forms/EeadABISlVmdJqgr2 "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
